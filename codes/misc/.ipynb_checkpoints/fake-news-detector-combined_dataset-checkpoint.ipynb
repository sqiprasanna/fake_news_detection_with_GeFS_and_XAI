{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:20.837079Z",
     "iopub.status.busy": "2023-05-04T00:32:20.836732Z",
     "iopub.status.idle": "2023-05-04T00:32:33.459697Z",
     "shell.execute_reply": "2023-05-04T00:32:33.458426Z",
     "shell.execute_reply.started": "2023-05-04T00:32:20.837048Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns \n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:33.462365Z",
     "iopub.status.busy": "2023-05-04T00:32:33.462045Z",
     "iopub.status.idle": "2023-05-04T00:32:36.270725Z",
     "shell.execute_reply": "2023-05-04T00:32:36.269689Z",
     "shell.execute_reply.started": "2023-05-04T00:32:33.462334Z"
    }
   },
   "outputs": [],
   "source": [
    "df_true = pd.read_csv(r\"C:\\Users\\Checkout\\Desktop\\SJSU\\sem2\\ISE244\\assignments\\project\\git-copy\\dataset\\True.csv\")\n",
    "df_fake = pd.read_csv(r\"C:\\Users\\Checkout\\Desktop\\SJSU\\sem2\\ISE244\\assignments\\project\\git-copy\\dataset\\Fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up a target variable and combining both the real and fake news datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:36.272568Z",
     "iopub.status.busy": "2023-05-04T00:32:36.272171Z",
     "iopub.status.idle": "2023-05-04T00:32:36.526765Z",
     "shell.execute_reply": "2023-05-04T00:32:36.525893Z",
     "shell.execute_reply.started": "2023-05-04T00:32:36.272514Z"
    }
   },
   "outputs": [],
   "source": [
    "df_true['target'] = 1\n",
    "df_fake['target'] = 0\n",
    "df_1 = pd.concat([df_true, df_fake]).reset_index(drop = True)\n",
    "df_1['original'] = df['title'] + ' ' + df['text']\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets take a note of the null values in the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:36.528463Z",
     "iopub.status.busy": "2023-05-04T00:32:36.528157Z",
     "iopub.status.idle": "2023-05-04T00:32:36.559328Z",
     "shell.execute_reply": "2023-05-04T00:32:36.557990Z",
     "shell.execute_reply.started": "2023-05-04T00:32:36.528428Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(r'../dataset/WELFake_Dataset.csv')\n",
    "df_2.dropna(subset = ['text', 'title'], inplace = True)\n",
    "df_2.rename(columns = {'label':'target'},inplace=True)\n",
    "df_2['text'] = df_2['title'] + ' ' + df['text']\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "sns.countplot(x = df_2['target'], palette = 'Set1', alpha = 0.8)\n",
    "plt.title('Distribution of Fake - 0 /Real - 1 News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title','text','target']\n",
    "df = pd.concat([df_1[cols], df_2[cols]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:36.562669Z",
     "iopub.status.busy": "2023-05-04T00:32:36.562359Z",
     "iopub.status.idle": "2023-05-04T00:32:36.574886Z",
     "shell.execute_reply": "2023-05-04T00:32:36.573785Z",
     "shell.execute_reply.started": "2023-05-04T00:32:36.562637Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:36.577660Z",
     "iopub.status.busy": "2023-05-04T00:32:36.577201Z",
     "iopub.status.idle": "2023-05-04T00:32:36.592347Z",
     "shell.execute_reply": "2023-05-04T00:32:36.591418Z",
     "shell.execute_reply.started": "2023-05-04T00:32:36.577616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the unmatching subjects to the same notation\n",
    "df_1.subject=df_1.subject.replace({'politics':'PoliticsNews','politicsNews':'PoliticsNews'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the distribution of Subjects between the True and Fake News? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:36.594386Z",
     "iopub.status.busy": "2023-05-04T00:32:36.593981Z",
     "iopub.status.idle": "2023-05-04T00:32:37.762598Z",
     "shell.execute_reply": "2023-05-04T00:32:37.761816Z",
     "shell.execute_reply.started": "2023-05-04T00:32:36.594341Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_tf_df=df.groupby('target').apply(lambda x:x['title'].count()).reset_index(name='Counts')\n",
    "sub_tf_df.target.replace({0:'False',1:'True'},inplace=True)\n",
    "display(sub_tf_df)\n",
    "fig = px.bar(sub_tf_df, x=\"target\", y=\"Counts\",\n",
    "             color='Counts', barmode='group',\n",
    "             height=400,width = 500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:37.764084Z",
     "iopub.status.busy": "2023-05-04T00:32:37.763660Z",
     "iopub.status.idle": "2023-05-04T00:32:37.854618Z",
     "shell.execute_reply": "2023-05-04T00:32:37.853623Z",
     "shell.execute_reply.started": "2023-05-04T00:32:37.764052Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_check=df_1.groupby('subject').apply(lambda x:x['title'].count()).reset_index(name='Counts')\n",
    "display(sub_check)\n",
    "fig=px.bar(sub_check,x='subject',y='Counts',color='Counts',title='Count of News Articles by Subject')\n",
    "fig.update_layout(width=800, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** Political News and World News hold the most domination counts in the data set that we have considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can the News Headline be enough to predict if the news if fake or not? Lets us see in the following analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:37.856855Z",
     "iopub.status.busy": "2023-05-04T00:32:37.856368Z",
     "iopub.status.idle": "2023-05-04T00:32:40.296970Z",
     "shell.execute_reply": "2023-05-04T00:32:40.296038Z",
     "shell.execute_reply.started": "2023-05-04T00:32:37.856807Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_title'] = df['title'].apply(preprocess)\n",
    "df['clean_title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:40.299030Z",
     "iopub.status.busy": "2023-05-04T00:32:40.298477Z",
     "iopub.status.idle": "2023-05-04T00:32:40.334136Z",
     "shell.execute_reply": "2023-05-04T00:32:40.333059Z",
     "shell.execute_reply.started": "2023-05-04T00:32:40.298982Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_joined_title']=df['clean_title'].apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Take a look at the Word Clouds for the Titles\n",
    "\n",
    "# Word Cloud using the Real News Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-05-04T00:32:40.335684Z",
     "iopub.status.busy": "2023-05-04T00:32:40.335384Z",
     "iopub.status.idle": "2023-05-04T00:33:00.737939Z",
     "shell.execute_reply": "2023-05-04T00:33:00.737190Z",
     "shell.execute_reply.started": "2023-05-04T00:32:40.335655Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (20,20)) \n",
    "# wc_r = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.target == 1].clean_joined_title))\n",
    "# plt.imshow(wc_r, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Top words in Real News Headline: \",list(wc_r.words_.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud using the Fake News Headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-05-04T00:33:00.739374Z",
     "iopub.status.busy": "2023-05-04T00:33:00.738984Z",
     "iopub.status.idle": "2023-05-04T00:33:21.671737Z",
     "shell.execute_reply": "2023-05-04T00:33:21.670942Z",
     "shell.execute_reply.started": "2023-05-04T00:33:00.739343Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (20,20)) \n",
    "# wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.target == 0].clean_joined_title))\n",
    "# plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Top words in Fake News Headline: \",list(wc.words_.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Look at the Count of Words Distribution in the Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = -1\n",
    "for doc in df.clean_joined_title:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in a title of news is =\", maxlen)\n",
    "fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined_title], nbins = 50, height= 500,width = 600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-05-04T00:33:21.673244Z",
     "iopub.status.busy": "2023-05-04T00:33:21.672849Z",
     "iopub.status.idle": "2023-05-04T00:33:32.529739Z",
     "shell.execute_reply": "2023-05-04T00:33:32.528675Z",
     "shell.execute_reply.started": "2023-05-04T00:33:21.673213Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = -1\n",
    "for doc in df[df.target == 0].clean_joined_title:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in a title of fake news is =\", maxlen)\n",
    "fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df[df.target == 0].clean_joined_title], nbins = 50, height= 500,width = 600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Subjects have received the most News Coverage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxlen = -1\n",
    "for doc in df[df.target == 1].clean_joined_title:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in a title of real news is =\", maxlen)\n",
    "fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df[df.target == 1].clean_joined_title], nbins = 50, height= 500,width = 600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "text_len = df[df[\"target\"] == 1][\"clean_joined_title\"].str.len()\n",
    "ax1.hist(text_len, color='green')\n",
    "ax1.set_title('Original Text')\n",
    "print(\"Average length of Original Text: \",text_len.mean())\n",
    "\n",
    "text_len = df[df[\"target\"] == 0][\"clean_joined_title\"].str.len()\n",
    "ax2.hist(text_len, color=\"red\")\n",
    "ax2.set_title(\"Fake Text\")\n",
    "print(\"Average length of Fake Text: \",text_len.mean())\n",
    "\n",
    "fig.suptitle(\"Characters in the Texts of Data Frame\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(corpus, n, g):\n",
    "    vector = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
    "    bag_of_words = vector.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "most_common_unigram_1 = dict(get_ngrams(df[df['target'] == 1][\"clean_joined_title\"], 10, 1))\n",
    "sns.barplot(x=list(most_common_unigram_1.values()), y=list(most_common_unigram_1.keys()))\n",
    "plt.title(\"Unigram Analysis - target = 1\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "most_common_unigram_0 = dict(get_ngrams(df[df['target'] == 0][\"clean_joined_title\"], 10, 1))\n",
    "sns.barplot(x=list(most_common_unigram_0.values()), y=list(most_common_unigram_0.keys()))\n",
    "plt.title(\"Unigram Analysis - target = 0\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "most_common_bigram_1 = dict(get_ngrams(df[df['target'] == 1][\"clean_joined_title\"], 10, 2))\n",
    "sns.barplot(x=list(most_common_bigram_1.values()), y=list(most_common_bigram_1.keys()))\n",
    "plt.title(\"Bigram Analysis - target = 1\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "most_common_bigram_0 = dict(get_ngrams(df[df['target'] == 0][\"clean_joined_title\"], 10, 2))\n",
    "sns.barplot(x=list(most_common_bigram_0.values()), y=list(most_common_bigram_0.keys()))\n",
    "plt.title(\"Bigram Analysis - target = 0\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "most_common_trigram_1 = dict(get_ngrams(df[df['target'] == 1][\"clean_joined_title\"], 10, 3))\n",
    "sns.barplot(x=list(most_common_trigram_1.values()), y=list(most_common_trigram_1.keys()))\n",
    "plt.title(\"TriGram Analysis - target = 1\")\n",
    "print(\"Most common tri grams in real news: \", most_common_trigram_1)\n",
    "plt.subplot(1, 2, 2)\n",
    "most_common_trigram_0 = dict(get_ngrams(df[df['target'] == 0][\"clean_joined_title\"], 10, 3))\n",
    "sns.barplot(x=list(most_common_trigram_0.values()), y=list(most_common_trigram_0.keys()))\n",
    "plt.title(\"TriGram Analysis - target = 0\")\n",
    "print(\"Most common tri grams in fake news: \", most_common_trigram_0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "word = df[df['target'] == 1]['clean_joined_title'].str.split().apply(lambda x: [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)), ax = ax1, color='green')\n",
    "ax1.set_title('Original text')\n",
    "\n",
    "word = df[df['target'] == 0]['clean_joined_title'].str.split().apply(lambda x: [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)), ax = ax2, color='red')\n",
    "ax2.set_title('Fake text')\n",
    "\n",
    "fig.suptitle('Average word length in each text')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df_1['date'] = pd.to_datetime(df_1['date'], format=\"%B %d, %Y\",errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the month, day, and year\n",
    "df_1['month'] = df_1['date'].dt.month\n",
    "df_1['day'] = df_1['date'].dt.day\n",
    "df_1['year'] = df_1['date'].dt.year\n",
    "\n",
    "# Group by month, day, and year and count the occurrences of fake news\n",
    "fake_news_counts_month = df_1[df_1['target'] == 0]['month'].value_counts()\n",
    "fake_news_counts_day = df_1[df_1['target'] == 0]['day'].value_counts()\n",
    "fake_news_counts_year = df_1[df_1['target'] == 0]['year'].value_counts()\n",
    "\n",
    "# Print the insights\n",
    "print(\"Fake News Published by Month:\")\n",
    "print(fake_news_counts_month)\n",
    "print(\"\\nFake News Published by Day:\")\n",
    "print(fake_news_counts_day)\n",
    "print(\"\\nFake News Published by Year:\")\n",
    "print(fake_news_counts_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:33:32.553032Z",
     "iopub.status.busy": "2023-05-04T00:33:32.552730Z",
     "iopub.status.idle": "2023-05-04T00:33:32.583085Z",
     "shell.execute_reply": "2023-05-04T00:33:32.581928Z",
     "shell.execute_reply.started": "2023-05-04T00:33:32.553003Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting value counts of month and year\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "# Plotting month\n",
    "axes[0].bar(fake_news_counts_day.index, fake_news_counts_day.values)\n",
    "axes[0].set_xticks(fake_news_counts_day.index)\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Fake News Published by Day')\n",
    "\n",
    "# Plotting month\n",
    "axes[1].bar(fake_news_counts_month.index, fake_news_counts_month.values)\n",
    "axes[1].set_xticks(fake_news_counts_month.index)\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Fake News Published by Month')\n",
    "\n",
    "# Plotting year\n",
    "axes[2].bar(fake_news_counts_year.index, fake_news_counts_year.values)\n",
    "axes[2].set_xticks(fake_news_counts_year.index)\n",
    "axes[2].set_xlabel('Year')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Fake News Published by Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:33:32.553032Z",
     "iopub.status.busy": "2023-05-04T00:33:32.552730Z",
     "iopub.status.idle": "2023-05-04T00:33:32.583085Z",
     "shell.execute_reply": "2023-05-04T00:33:32.581928Z",
     "shell.execute_reply.started": "2023-05-04T00:33:32.553003Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:33:32.585237Z",
     "iopub.status.busy": "2023-05-04T00:33:32.584769Z",
     "iopub.status.idle": "2023-05-04T00:33:39.419070Z",
     "shell.execute_reply": "2023-05-04T00:33:39.417971Z",
     "shell.execute_reply.started": "2023-05-04T00:33:32.585186Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus = [text.split() for text in df['clean_joined_title']]\n",
    "emb_size = 100\n",
    "# Train Word2Vec model\n",
    "model_w2v = Word2Vec(sentences=corpus, vector_size=emb_size, window=5, min_count=1, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116435, 5)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_joined_title'] = df['clean_joined_title'].astype(str)\n",
    "df = df.dropna(subset=['clean_joined_title']).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:33:39.420805Z",
     "iopub.status.busy": "2023-05-04T00:33:39.420438Z",
     "iopub.status.idle": "2023-05-04T00:34:06.652350Z",
     "shell.execute_reply": "2023-05-04T00:34:06.651529Z",
     "shell.execute_reply.started": "2023-05-04T00:33:39.420768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116435, 100)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_text_vec = []\n",
    "for text in df['clean_joined_title']:\n",
    "    vec = np.zeros(emb_size)\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token in model_w2v.wv:\n",
    "            vec += model_w2v.wv[token]\n",
    "    meta_text_vec.append(vec)\n",
    "meta_text_vec = np.array(meta_text_vec)\n",
    "meta_text_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model_w2v.wv.index_to_key\n",
    "# words,len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116435, 100)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116435, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['target']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:34:06.656996Z",
     "iopub.status.busy": "2023-05-04T00:34:06.656023Z",
     "iopub.status.idle": "2023-05-04T00:34:06.720133Z",
     "shell.execute_reply": "2023-05-04T00:34:06.719165Z",
     "shell.execute_reply.started": "2023-05-04T00:34:06.656943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_91</th>\n",
       "      <th>embed_92</th>\n",
       "      <th>embed_93</th>\n",
       "      <th>embed_94</th>\n",
       "      <th>embed_95</th>\n",
       "      <th>embed_96</th>\n",
       "      <th>embed_97</th>\n",
       "      <th>embed_98</th>\n",
       "      <th>embed_99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.570240</td>\n",
       "      <td>-2.803373</td>\n",
       "      <td>-2.554232</td>\n",
       "      <td>-0.106434</td>\n",
       "      <td>0.250771</td>\n",
       "      <td>-1.301584</td>\n",
       "      <td>-2.329370</td>\n",
       "      <td>-0.392372</td>\n",
       "      <td>4.628004</td>\n",
       "      <td>-3.979166</td>\n",
       "      <td>...</td>\n",
       "      <td>3.328781</td>\n",
       "      <td>4.353628</td>\n",
       "      <td>3.194135</td>\n",
       "      <td>1.394847</td>\n",
       "      <td>0.248404</td>\n",
       "      <td>-0.223095</td>\n",
       "      <td>1.888524</td>\n",
       "      <td>0.157861</td>\n",
       "      <td>2.502262</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.908017</td>\n",
       "      <td>2.455392</td>\n",
       "      <td>-2.110239</td>\n",
       "      <td>1.305294</td>\n",
       "      <td>0.582295</td>\n",
       "      <td>-2.320805</td>\n",
       "      <td>-2.858762</td>\n",
       "      <td>0.900695</td>\n",
       "      <td>4.572054</td>\n",
       "      <td>-0.224374</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264421</td>\n",
       "      <td>2.602593</td>\n",
       "      <td>-0.417223</td>\n",
       "      <td>1.416479</td>\n",
       "      <td>3.616037</td>\n",
       "      <td>0.233119</td>\n",
       "      <td>1.427202</td>\n",
       "      <td>-3.339051</td>\n",
       "      <td>3.654810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.448401</td>\n",
       "      <td>-3.275209</td>\n",
       "      <td>-1.026117</td>\n",
       "      <td>-2.486848</td>\n",
       "      <td>1.204069</td>\n",
       "      <td>-4.548593</td>\n",
       "      <td>-0.637636</td>\n",
       "      <td>-2.377687</td>\n",
       "      <td>6.101851</td>\n",
       "      <td>-3.428769</td>\n",
       "      <td>...</td>\n",
       "      <td>3.263841</td>\n",
       "      <td>3.248214</td>\n",
       "      <td>3.120166</td>\n",
       "      <td>1.702474</td>\n",
       "      <td>-1.572957</td>\n",
       "      <td>1.035061</td>\n",
       "      <td>4.367816</td>\n",
       "      <td>0.679509</td>\n",
       "      <td>3.884487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.408945</td>\n",
       "      <td>0.669949</td>\n",
       "      <td>-3.984049</td>\n",
       "      <td>2.098730</td>\n",
       "      <td>1.751097</td>\n",
       "      <td>-4.748281</td>\n",
       "      <td>-3.382129</td>\n",
       "      <td>-3.584832</td>\n",
       "      <td>4.995571</td>\n",
       "      <td>1.408581</td>\n",
       "      <td>...</td>\n",
       "      <td>3.665572</td>\n",
       "      <td>1.953220</td>\n",
       "      <td>-2.979264</td>\n",
       "      <td>4.541581</td>\n",
       "      <td>0.762408</td>\n",
       "      <td>5.407749</td>\n",
       "      <td>4.735015</td>\n",
       "      <td>-5.043454</td>\n",
       "      <td>6.737858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.590725</td>\n",
       "      <td>-0.240896</td>\n",
       "      <td>-0.613958</td>\n",
       "      <td>0.896606</td>\n",
       "      <td>-0.125526</td>\n",
       "      <td>-2.523806</td>\n",
       "      <td>0.075854</td>\n",
       "      <td>-0.068514</td>\n",
       "      <td>2.739620</td>\n",
       "      <td>-2.833008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795078</td>\n",
       "      <td>1.774929</td>\n",
       "      <td>-0.490356</td>\n",
       "      <td>1.994058</td>\n",
       "      <td>1.871106</td>\n",
       "      <td>0.019543</td>\n",
       "      <td>-0.384379</td>\n",
       "      <td>0.198380</td>\n",
       "      <td>3.747008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0 -3.570240 -2.803373 -2.554232 -0.106434  0.250771 -1.301584 -2.329370   \n",
       "1 -2.908017  2.455392 -2.110239  1.305294  0.582295 -2.320805 -2.858762   \n",
       "2 -1.448401 -3.275209 -1.026117 -2.486848  1.204069 -4.548593 -0.637636   \n",
       "3 -3.408945  0.669949 -3.984049  2.098730  1.751097 -4.748281 -3.382129   \n",
       "4 -2.590725 -0.240896 -0.613958  0.896606 -0.125526 -2.523806  0.075854   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_91  embed_92  embed_93  embed_94  \\\n",
       "0 -0.392372  4.628004 -3.979166  ...  3.328781  4.353628  3.194135  1.394847   \n",
       "1  0.900695  4.572054 -0.224374  ...  1.264421  2.602593 -0.417223  1.416479   \n",
       "2 -2.377687  6.101851 -3.428769  ...  3.263841  3.248214  3.120166  1.702474   \n",
       "3 -3.584832  4.995571  1.408581  ...  3.665572  1.953220 -2.979264  4.541581   \n",
       "4 -0.068514  2.739620 -2.833008  ...  0.795078  1.774929 -0.490356  1.994058   \n",
       "\n",
       "   embed_95  embed_96  embed_97  embed_98  embed_99  target  \n",
       "0  0.248404 -0.223095  1.888524  0.157861  2.502262       1  \n",
       "1  3.616037  0.233119  1.427202 -3.339051  3.654810       1  \n",
       "2 -1.572957  1.035061  4.367816  0.679509  3.884487       1  \n",
       "3  0.762408  5.407749  4.735015 -5.043454  6.737858       1  \n",
       "4  1.871106  0.019543 -0.384379  0.198380  3.747008       1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embed = pd.DataFrame(meta_text_vec,columns = ['embed_{}'.format(i) for i in range(meta_text_vec.shape[1])])\n",
    "final_df = pd.concat([df_embed, df[['target']]],axis = 1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    58509\n",
       "1    57926\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:36:30.157226Z",
     "iopub.status.busy": "2023-05-04T00:36:30.156837Z",
     "iopub.status.idle": "2023-05-04T00:36:30.166365Z",
     "shell.execute_reply": "2023-05-04T00:36:30.165258Z",
     "shell.execute_reply.started": "2023-05-04T00:36:30.157192Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "classifiers = {'Logistic': LogisticRegression(max_iter=1000),\n",
    "               'KNN': KNeighborsClassifier(),\n",
    "               'RandomForest':RandomForestClassifier(),\n",
    "               'XGBOOST': xgb.XGBClassifier(n_estimators=300, random_state=0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:36:31.098518Z",
     "iopub.status.busy": "2023-05-04T00:36:31.098162Z",
     "iopub.status.idle": "2023-05-04T00:37:51.009250Z",
     "shell.execute_reply": "2023-05-04T00:37:51.008312Z",
     "shell.execute_reply.started": "2023-05-04T00:36:31.098487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic\n",
      "Accuracy: 0.5706138994812601\n",
      "Precision: 0.55480672830567\n",
      "Recall: 0.688532522292113\n",
      "F1 score: 0.6144782702569321\n",
      "\n",
      "Model: KNN\n",
      "Accuracy: 0.39705932872994604\n",
      "Precision: 0.39859266079179273\n",
      "Recall: 0.4189534803345545\n",
      "F1 score: 0.40851952953863785\n",
      "\n",
      "Model: RandomForest\n",
      "Accuracy: 0.30842694699233913\n",
      "Precision: 0.3132418886837246\n",
      "Recall: 0.32833344853805213\n",
      "F1 score: 0.3206101717795552\n",
      "\n",
      "[00:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-6c7eaaa46c17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1259\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         )\n\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    194\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[0;32m    197\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1680\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1681\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1682\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1684\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def split(df,label):\n",
    "    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n",
    "    return X_tr, X_te, Y_tr, Y_te\n",
    "data_bc = final_df.drop(columns='target')\n",
    "label_bc = final_df['target']\n",
    "X_train, X_test, y_train, y_test = split(data_bc, label_bc)\n",
    "\n",
    "for key in classifiers:\n",
    "    classifiers[key].fit(X_train, y_train)\n",
    "    y_pred = classifiers[key].predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {key}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a SHAP explainer object for the logistic regression model\n",
    "lr_explainer = shap.LinearExplainer(classifiers['Logistic'], X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values for a sample of the test data\n",
    "lr_shap_values = lr_explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the SHAP values for the logistic regression model\n",
    "shap.summary_plot(lr_shap_values, X_test,max_display =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_explainer = shap.Explainer(classifiers['XGBOOST'])\n",
    "xgb_shap_values = xgb_explainer(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(xgb_shap_values, X_test,max_display =10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP positive/negative impact plot\n",
    "shap.plots.bar(xgb_explainer(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(xgb_shap_values, X_test, plot_type=\"bar\",max_display =10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:43:45.671910Z",
     "iopub.status.busy": "2023-05-04T00:43:45.671497Z",
     "iopub.status.idle": "2023-05-04T00:43:59.794954Z",
     "shell.execute_reply": "2023-05-04T00:43:59.794013Z",
     "shell.execute_reply.started": "2023-05-04T00:43:45.671874Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature extraction using Word2Vec\n",
    "sentences = [nltk.word_tokenize(text) for text in df['clean_joined_title']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, min_count=1)\n",
    "# create embeddings\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:48:59.423762Z",
     "iopub.status.busy": "2023-05-04T00:48:59.423112Z",
     "iopub.status.idle": "2023-05-04T00:48:59.429367Z",
     "shell.execute_reply": "2023-05-04T00:48:59.428232Z",
     "shell.execute_reply.started": "2023-05-04T00:48:59.423681Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(word2vec_model.wv.key_to_index)\n",
    "\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(df['clean_joined_title'].values)\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_joined_title'].values)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:44:05.383702Z",
     "iopub.status.busy": "2023-05-04T00:44:05.383344Z",
     "iopub.status.idle": "2023-05-04T00:44:05.404027Z",
     "shell.execute_reply": "2023-05-04T00:44:05.403099Z",
     "shell.execute_reply.started": "2023-05-04T00:44:05.383670Z"
    }
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, df['target'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:49:20.754752Z",
     "iopub.status.busy": "2023-05-04T00:49:20.754059Z",
     "iopub.status.idle": "2023-05-04T00:49:21.052103Z",
     "shell.execute_reply": "2023-05-04T00:49:21.051218Z",
     "shell.execute_reply.started": "2023-05-04T00:49:20.754696Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = tf.keras.layers.Embedding(vocabulary_size+1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)(input_tensor)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
    "conv1d_layer = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(dropout_layer)\n",
    "maxpooling1d_layer = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1d_layer)\n",
    "lstm_layer = tf.keras.layers.LSTM(100)(maxpooling1d_layer)\n",
    "output_tensor = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "cnn_lstm_model = tf.keras.models.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "cnn_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:49:24.468269Z",
     "iopub.status.busy": "2023-05-04T00:49:24.467902Z",
     "iopub.status.idle": "2023-05-04T00:54:19.584134Z",
     "shell.execute_reply": "2023-05-04T00:54:19.583100Z",
     "shell.execute_reply.started": "2023-05-04T00:49:24.468236Z"
    }
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "history = cnn_lstm_model.fit(X_train, np.array(y_train), epochs=10, batch_size=256,\n",
    "#                                callbacks = [early_stopping],\n",
    "                             validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model.save('lstm_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:54:19.587462Z",
     "iopub.status.busy": "2023-05-04T00:54:19.586699Z",
     "iopub.status.idle": "2023-05-04T00:54:19.757306Z",
     "shell.execute_reply": "2023-05-04T00:54:19.756157Z",
     "shell.execute_reply.started": "2023-05-04T00:54:19.587382Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train and validation loss\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T00:54:19.759659Z",
     "iopub.status.busy": "2023-05-04T00:54:19.758992Z",
     "iopub.status.idle": "2023-05-04T00:54:23.481972Z",
     "shell.execute_reply": "2023-05-04T00:54:23.480787Z",
     "shell.execute_reply.started": "2023-05-04T00:54:19.759609Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred_prob = cnn_lstm_model.predict(X_test)\n",
    "\n",
    "# Get the class labels as integers\n",
    "# y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_pred = [1 if each > 0.5 else 0 for each in y_pred_prob ]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
